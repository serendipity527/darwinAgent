{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-16T11:45:46.912233Z"
    }
   },
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "简单的AI对话系统 - 单文件版本\n",
    "使用本地Ollama部署的Gemma 3模型\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import httpx\n",
    "import gradio as gr\n",
    "from typing import List, Dict, Any, Optional\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "# ===================== 配置部分 =====================\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"  # Ollama默认地址\n",
    "MODEL_NAME = \"gemma:3b\"  # 使用的模型名称，根据实际部署情况调整\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"你是一个友好、智能的AI助手。\n",
    "你的目标是提供有用、准确的回答，并与用户进行自然的对话。\n",
    "\n",
    "请遵循以下原则：\n",
    "1. 保持回答简洁明了\n",
    "2. 如果不确定答案，坦诚承认\n",
    "3. 避免有害、不适当或违法的内容\n",
    "4. 尊重用户的隐私\n",
    "5. 保持友好和专业的语气\n",
    "\n",
    "当用户问你问题时，请尽可能地提供帮助。\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ===================== 模型服务部分 =====================\n",
    "class OllamaService:\n",
    "    \"\"\"\n",
    "    Ollama服务类，用于与本地Ollama API交互\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: str = MODEL_NAME,\n",
    "            base_url: str = OLLAMA_BASE_URL,\n",
    "            temperature: float = 0.7,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        初始化Ollama服务\n",
    "\n",
    "        Args:\n",
    "            model: 模型名称\n",
    "            base_url: Ollama API基础URL\n",
    "            temperature: 温度参数\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.base_url = base_url\n",
    "        self.temperature = temperature\n",
    "\n",
    "    async def check_model_availability(self) -> bool:\n",
    "        \"\"\"\n",
    "        检查模型是否可用\n",
    "\n",
    "        Returns:\n",
    "            bool: 模型是否可用\n",
    "        \"\"\"\n",
    "        try:\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(f\"{self.base_url}/api/tags\")\n",
    "                models = response.json().get(\"models\", [])\n",
    "                return any(m.get(\"name\") == self.model for m in models)\n",
    "        except Exception as e:\n",
    "            print(f\"检查模型可用性时出错: {e}\")\n",
    "            return False\n",
    "\n",
    "    @retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=10))\n",
    "    async def generate_text(self, messages: List[Dict[str, str]]) -> str:\n",
    "        \"\"\"\n",
    "        生成文本\n",
    "\n",
    "        Args:\n",
    "            messages: 消息列表\n",
    "\n",
    "        Returns:\n",
    "            str: 生成的文本\n",
    "        \"\"\"\n",
    "        # 转换消息格式以适应Ollama API\n",
    "        prompt = \"\"\n",
    "        system_content = \"\"\n",
    "\n",
    "        for msg in messages:\n",
    "            if msg[\"role\"] == \"system\":\n",
    "                system_content = msg[\"content\"]\n",
    "            elif msg[\"role\"] == \"user\":\n",
    "                prompt += f\"用户: {msg['content']}\\n\"\n",
    "            elif msg[\"role\"] == \"assistant\":\n",
    "                prompt += f\"助手: {msg['content']}\\n\"\n",
    "\n",
    "        # 最后添加助手前缀\n",
    "        prompt += \"助手: \"\n",
    "\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"prompt\": prompt,\n",
    "            \"system\": system_content,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"stream\": False  # 不使用流式响应\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.post(\n",
    "                    f\"{self.base_url}/api/generate\",\n",
    "                    json=payload,\n",
    "                    timeout=60.0\n",
    "                )\n",
    "\n",
    "                response.raise_for_status()\n",
    "                result = response.json()\n",
    "\n",
    "                return result.get(\"response\", \"\")\n",
    "        except Exception as e:\n",
    "            print(f\"生成文本时出错: {e}\")\n",
    "            return f\"抱歉，生成回复时出现错误: {str(e)}\"\n",
    "\n",
    "\n",
    "# ===================== 聊天代理部分 =====================\n",
    "class ChatAgent:\n",
    "    \"\"\"\n",
    "    聊天代理类，处理对话逻辑\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Dict[str, Any] = None):\n",
    "        \"\"\"\n",
    "        初始化聊天代理\n",
    "\n",
    "        Args:\n",
    "            config: 代理配置\n",
    "        \"\"\"\n",
    "        self.config = config or {}\n",
    "        self.history = []\n",
    "        self.ollama_service = OllamaService(\n",
    "            model=config.get(\"model\", MODEL_NAME),\n",
    "            temperature=config.get(\"temperature\", 0.7)\n",
    "        )\n",
    "        self.system_prompt = config.get(\"system_prompt\", DEFAULT_SYSTEM_PROMPT)\n",
    "\n",
    "    async def process(self, user_input: str, **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        处理用户输入并返回响应\n",
    "\n",
    "        Args:\n",
    "            user_input: 用户输入\n",
    "            **kwargs: 其他参数\n",
    "\n",
    "        Returns:\n",
    "            str: 代理响应\n",
    "        \"\"\"\n",
    "        # 准备消息列表\n",
    "        messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "\n",
    "        # 添加历史消息\n",
    "        for item in self.history:\n",
    "            messages.append({\"role\": \"user\", \"content\": item[\"user\"]})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": item[\"assistant\"]})\n",
    "\n",
    "        # 添加当前用户输入\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # 调用LLM获取响应\n",
    "        response = await self.ollama_service.generate_text(messages)\n",
    "\n",
    "        # 添加到历史\n",
    "        self.add_to_history(user_input, response)\n",
    "\n",
    "        return response\n",
    "\n",
    "    def add_to_history(self, user_input: str, response: str):\n",
    "        \"\"\"\n",
    "        将对话添加到历史记录\n",
    "\n",
    "        Args:\n",
    "            user_input: 用户输入\n",
    "            response: 代理响应\n",
    "        \"\"\"\n",
    "        self.history.append({\"user\": user_input, \"assistant\": response})\n",
    "\n",
    "    def get_history(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        获取对话历史\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, str]]: 对话历史\n",
    "        \"\"\"\n",
    "        return self.history\n",
    "\n",
    "    def clear_history(self):\n",
    "        \"\"\"\n",
    "        清空对话历史\n",
    "        \"\"\"\n",
    "        self.history = []\n",
    "\n",
    "\n",
    "# ===================== UI部分 =====================\n",
    "def create_chat_interface():\n",
    "    \"\"\"\n",
    "    创建聊天界面\n",
    "\n",
    "    Returns:\n",
    "        gr.Blocks: Gradio界面\n",
    "    \"\"\"\n",
    "    # 创建聊天代理\n",
    "    chat_agent = ChatAgent(config={\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"temperature\": 0.7,\n",
    "        \"system_prompt\": DEFAULT_SYSTEM_PROMPT\n",
    "    })\n",
    "\n",
    "    with gr.Blocks(title=\"Gemma 3 聊天助手\") as demo:\n",
    "        gr.Markdown(\"# Gemma 3 聊天助手\")\n",
    "        gr.Markdown(\"使用本地Ollama部署的Gemma 3模型\")\n",
    "\n",
    "        # 聊天历史\n",
    "        chatbot = gr.Chatbot(\n",
    "            height=500,\n",
    "            show_copy_button=True,\n",
    "            elem_id=\"chatbot\",\n",
    "            label=\"对话历史\"\n",
    "        )\n",
    "\n",
    "        # 输入区域\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=8):\n",
    "                user_input = gr.Textbox(\n",
    "                    placeholder=\"在这里输入您的问题...\",\n",
    "                    label=\"用户输入\",\n",
    "                    lines=2,\n",
    "                    elem_id=\"user-input\"\n",
    "                )\n",
    "\n",
    "            with gr.Column(scale=1):\n",
    "                submit_btn = gr.Button(\"发送\", variant=\"primary\")\n",
    "                clear_btn = gr.Button(\"清空\")\n",
    "\n",
    "        # 高级选项\n",
    "        with gr.Accordion(\"高级选项\", open=False):\n",
    "            with gr.Row():\n",
    "                temperature = gr.Slider(\n",
    "                    minimum=0.0,\n",
    "                    maximum=1.0,\n",
    "                    value=0.7,\n",
    "                    step=0.1,\n",
    "                    label=\"温度参数\"\n",
    "                )\n",
    "\n",
    "                model_name = gr.Dropdown(\n",
    "                    choices=[\"gemma:3b\", \"gemma:7b\", \"gemma:2b-instruct\"],\n",
    "                    value=MODEL_NAME,\n",
    "                    label=\"模型选择\"\n",
    "                )\n",
    "\n",
    "            system_prompt = gr.Textbox(\n",
    "                value=DEFAULT_SYSTEM_PROMPT,\n",
    "                label=\"系统提示词\",\n",
    "                lines=5\n",
    "            )\n",
    "\n",
    "        # 状态显示\n",
    "        status = gr.Markdown(\"状态: 就绪\")\n",
    "\n",
    "        # 辅助函数\n",
    "        async def check_model():\n",
    "            is_available = await chat_agent.ollama_service.check_model_availability()\n",
    "            if is_available:\n",
    "                return \"状态: 模型已加载，可以开始对话\"\n",
    "            else:\n",
    "                return f\"状态: 警告 - 模型 {chat_agent.ollama_service.model} 可能未加载，请确保Ollama正在运行并已下载该模型\"\n",
    "\n",
    "        async def chat_response(\n",
    "                user_input: str,\n",
    "                history: List[List[str]],\n",
    "                temp: float,\n",
    "                model: str,\n",
    "                sys_prompt: str\n",
    "        ):\n",
    "            if not user_input.strip():\n",
    "                yield history, \"\"\n",
    "                return  # 不返回值，只结束函数\n",
    "\n",
    "            # 更新历史\n",
    "            history.append([user_input, None])\n",
    "            yield history, \"\"\n",
    "\n",
    "            # 更新代理配置\n",
    "            chat_agent.ollama_service.temperature = temp\n",
    "            chat_agent.ollama_service.model = model\n",
    "            chat_agent.system_prompt = sys_prompt\n",
    "\n",
    "            # 获取响应\n",
    "            response = await chat_agent.process(user_input)\n",
    "\n",
    "            # 更新历史\n",
    "            history[-1][1] = response\n",
    "            yield history, \"\"\n",
    "\n",
    "        def clear_conversation():\n",
    "            # 清空代理历史\n",
    "            chat_agent.clear_history()\n",
    "            return [], \"\"\n",
    "\n",
    "        # 注册回调\n",
    "        submit_btn.click(\n",
    "            fn=chat_response,\n",
    "            inputs=[user_input, chatbot, temperature, model_name, system_prompt],\n",
    "            outputs=[chatbot, user_input]\n",
    "        )\n",
    "\n",
    "        clear_btn.click(\n",
    "            fn=clear_conversation,\n",
    "            inputs=[],\n",
    "            outputs=[chatbot, user_input]\n",
    "        )\n",
    "\n",
    "        # 页面加载时检查模型状态\n",
    "        demo.load(\n",
    "            fn=check_model,\n",
    "            inputs=None,\n",
    "            outputs=status\n",
    "        )\n",
    "\n",
    "    return demo\n",
    "\n",
    "\n",
    "# ===================== 主函数 =====================\n",
    "def main():\n",
    "    \"\"\"\n",
    "    主函数\n",
    "    \"\"\"\n",
    "    # 创建并启动Gradio应用\n",
    "    demo = create_chat_interface()\n",
    "\n",
    "    # 设置启动参数\n",
    "    port = int(os.getenv(\"GRADIO_PORT\", 7860))\n",
    "    share = os.getenv(\"GRADIO_SHARE\", \"false\").lower() == \"true\"\n",
    "\n",
    "    # 启动应用\n",
    "    demo.launch(\n",
    "        server_name=\"0.0.0.0\",  # 允许外部访问\n",
    "        server_port=port,\n",
    "        share=share,\n",
    "        debug=True\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edy/Documents/Yihu/darwinAgent/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/8k/msfr41dj559byywk1kcjpm9h0000gn/T/ipykernel_27527/497836430.py:222: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
