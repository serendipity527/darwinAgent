{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T12:32:29.462007Z",
     "start_time": "2025-07-21T12:32:28.773565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "ä½¿ç”¨ LCEL (LangChain Expression Language) å®ç°å¯¹è¯è®°å¿†\n",
    "å±•ç¤ºå¤šç§åœ¨ LCEL ä¸­ç®¡ç†å¯¹è¯å†å²çš„æ–¹æ³•\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import Dict, List, Any, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "print(\"LCEL å¯¹è¯è®°å¿†å®ç°æ¼”ç¤º\")\n",
    "print(\"=\" * 40)\n"
   ],
   "id": "962b4a5c29d56c13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCEL å¯¹è¯è®°å¿†å®ç°æ¼”ç¤º\n",
      "========================================\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T12:54:48.135978Z",
     "start_time": "2025-07-21T12:54:48.085913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===================== é…ç½®éƒ¨åˆ† =====================\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "OLLAMA_MODEL = \"gemma3:4b\"\n",
    "\n",
    "def create_llm():\n",
    "    \"\"\"åˆ›å»ºLLMå®ä¾‹\"\"\"\n",
    "    return OllamaLLM(\n",
    "        base_url=OLLAMA_BASE_URL,\n",
    "        model=OLLAMA_MODEL,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "llm = create_llm()\n",
    "print(f\"ä½¿ç”¨æ¨¡å‹: {OLLAMA_MODEL}\")\n"
   ],
   "id": "59a2f706d46042fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½¿ç”¨æ¨¡å‹: gemma3:4b\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ===================== æ–¹æ³•1: æ‰‹åŠ¨ç®¡ç†å¯¹è¯å†å² =====================\n",
    "\n",
    "print(\"\\n1. æ‰‹åŠ¨ç®¡ç†å¯¹è¯å†å²\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "class ManualMemoryChain:\n",
    "    \"\"\"æ‰‹åŠ¨ç®¡ç†å¯¹è¯å†å²çš„é“¾\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.history: List[BaseMessage] = []\n",
    "        \n",
    "        # åˆ›å»ºæç¤ºæ¨¡æ¿\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿè®°ä½å¯¹è¯å†å²ã€‚\"),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            (\"human\", \"{input}\")\n",
    "        ])\n",
    "        \n",
    "        # åˆ›å»ºé“¾\n",
    "        self.chain = self.prompt | self.llm | StrOutputParser()\n",
    "    \n",
    "    def invoke(self, user_input: str) -> str:\n",
    "        \"\"\"è°ƒç”¨é“¾å¹¶æ›´æ–°å†å²\"\"\"\n",
    "        # è°ƒç”¨é“¾\n",
    "        response = self.chain.invoke({\n",
    "            \"history\": self.history,\n",
    "            \"input\": user_input\n",
    "        })\n",
    "        \n",
    "        # æ›´æ–°å†å²\n",
    "        self.history.append(HumanMessage(content=user_input))\n",
    "        self.history.append(AIMessage(content=response))\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"æ¸…ç©ºå†å²\"\"\"\n",
    "        self.history = []\n",
    "    \n",
    "    def get_history(self) -> List[BaseMessage]:\n",
    "        \"\"\"è·å–å†å²\"\"\"\n",
    "        return self.history.copy()\n",
    "\n",
    "# æ¼”ç¤ºæ‰‹åŠ¨è®°å¿†ç®¡ç†\n",
    "def demo_manual_memory():\n",
    "    \"\"\"æ¼”ç¤ºæ‰‹åŠ¨è®°å¿†ç®¡ç†\"\"\"\n",
    "    print(\"\\næ‰‹åŠ¨è®°å¿†ç®¡ç†æ¼”ç¤º:\")\n",
    "    \n",
    "    chain = ManualMemoryChain(llm)\n",
    "    \n",
    "    conversations = [\n",
    "        \"ä½ å¥½ï¼Œæˆ‘å«å°æ˜ï¼Œæ˜¯ä¸€åç¨‹åºå‘˜\",\n",
    "        \"æˆ‘åœ¨ä¸Šæµ·å·¥ä½œï¼Œä¸»è¦åšPythonå¼€å‘\",\n",
    "        \"ä½ è¿˜è®°å¾—æˆ‘çš„åå­—å—ï¼Ÿ\",\n",
    "        \"æˆ‘åœ¨å“ªä¸ªåŸå¸‚å·¥ä½œï¼Ÿ\"\n",
    "    ]\n",
    "    \n",
    "    for i, user_input in enumerate(conversations, 1):\n",
    "        try:\n",
    "            response = chain.invoke(user_input)\n",
    "            print(f\"\\nç¬¬{i}è½®:\")\n",
    "            print(f\"ç”¨æˆ·: {user_input}\")\n",
    "            print(f\"AI: {response}\")\n",
    "        except Exception as e:\n",
    "            print(f\"é”™è¯¯: {e}\")\n",
    "    \n",
    "    print(f\"\\nå†å²è®°å½•æ•°é‡: {len(chain.get_history())} æ¡æ¶ˆæ¯\")\n",
    "\n",
    "demo_manual_memory()\n"
   ],
   "id": "7fd10b6d3e3db875"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T12:57:48.903306Z",
     "start_time": "2025-07-21T12:57:29.410052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===================== æ–¹æ³•2: ä½¿ç”¨ RunnableWithMessageHistory =====================\n",
    "\n",
    "print(\"\\n\\n2. ä½¿ç”¨ RunnableWithMessageHistory\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# å­˜å‚¨ä¼šè¯å†å²çš„å­—å…¸\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    \"\"\"è·å–ä¼šè¯å†å²\"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# åˆ›å»ºåŸºç¡€é“¾\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ã€‚è¯·æ ¹æ®å¯¹è¯å†å²è¿›è¡Œå›å¤ã€‚\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "base_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# æ·»åŠ æ¶ˆæ¯å†å²\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    base_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "def demo_runnable_with_history():\n",
    "    \"\"\"æ¼”ç¤º RunnableWithMessageHistory\"\"\"\n",
    "    print(\"\\nRunnableWithMessageHistory æ¼”ç¤º:\")\n",
    "    \n",
    "    # é…ç½®ä¼šè¯\n",
    "    config = {\"configurable\": {\"session_id\": \"demo_session\"}}\n",
    "    \n",
    "    conversations = [\n",
    "        \"ä½ å¥½ï¼Œæˆ‘æ˜¯æåï¼Œæˆ‘å–œæ¬¢é˜…è¯»\",\n",
    "        \"æˆ‘æœ€å–œæ¬¢çš„ä¹¦æ˜¯ã€Šä¸‰ä½“ã€‹\",\n",
    "        \"ä½ è¿˜è®°å¾—æˆ‘çš„åå­—å—ï¼Ÿ\",\n",
    "        \"æˆ‘å–œæ¬¢ä»€ä¹ˆä¹¦ï¼Ÿ\"\n",
    "    ]\n",
    "    \n",
    "    for i, user_input in enumerate(conversations, 1):\n",
    "        try:\n",
    "            response = chain_with_history.invoke(\n",
    "                {\"input\": user_input},\n",
    "                config=config\n",
    "            )\n",
    "            print(f\"\\nç¬¬{i}è½®:\")\n",
    "            print(f\"ç”¨æˆ·: {user_input}\")\n",
    "            print(f\"AI: {response}\")\n",
    "        except Exception as e:\n",
    "            print(f\"é”™è¯¯: {e}\")\n",
    "    \n",
    "    # æ˜¾ç¤ºå†å²\n",
    "    history = get_session_history(\"demo_session\")\n",
    "    print(f\"\\nä¼šè¯å†å²: {len(history.messages)} æ¡æ¶ˆæ¯\")\n",
    "\n",
    "demo_runnable_with_history()\n"
   ],
   "id": "35ddf3d4c87aadd0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "2. ä½¿ç”¨ RunnableWithMessageHistory\n",
      "----------------------------------------\n",
      "\n",
      "RunnableWithMessageHistory æ¼”ç¤º:\n",
      "\n",
      "ç¬¬1è½®:\n",
      "ç”¨æˆ·: ä½ å¥½ï¼Œæˆ‘æ˜¯æåï¼Œæˆ‘å–œæ¬¢é˜…è¯»\n",
      "AI: ä½ å¥½æåï¼Œå¾ˆé«˜å…´è®¤è¯†ä½ ï¼ å¾ˆé«˜å…´çŸ¥é“ä½ å–œæ¬¢é˜…è¯»ï¼Œé‚£çœŸæ˜¯å¤ªæ£’äº†ï¼ ä½ å¹³æ—¶å–œæ¬¢è¯»ä»€ä¹ˆç±»å‹çš„ä¹¦å‘¢ï¼Ÿ ğŸ˜Š\n",
      "\n",
      "\n",
      "ç¬¬2è½®:\n",
      "ç”¨æˆ·: æˆ‘æœ€å–œæ¬¢çš„ä¹¦æ˜¯ã€Šä¸‰ä½“ã€‹\n",
      "AI: AI: ã€Šä¸‰ä½“ã€‹ï¼å“‡ï¼Œåˆ˜æ…ˆæ¬£çš„ã€Šä¸‰ä½“ã€‹ç»å¯¹æ˜¯ç§‘å¹»ç»å…¸ï¼ ä½ å–œæ¬¢å®ƒçš„ä»€ä¹ˆå‘¢ï¼Ÿ æ˜¯åˆ˜æ…ˆæ¬£çš„æƒ³è±¡åŠ›ï¼Œè¿˜æ˜¯æ•…äº‹çš„æ·±åº¦ï¼Œè¿˜æ˜¯æ•´ä¸ªå®‡å®™çš„å®å¤§å™äº‹ï¼Ÿ ğŸ˜Š\n",
      "\n",
      "\n",
      "ç¬¬3è½®:\n",
      "ç”¨æˆ·: ä½ è¿˜è®°å¾—æˆ‘çš„åå­—å—ï¼Ÿ\n",
      "AI: AI: å½“ç„¶è®°å¾—ï¼æåï¼Œå¾ˆé«˜å…´å†æ¬¡å’Œä½ èŠå¤© ğŸ˜Š ä½ ä»Šå¤©è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿ\n",
      "\n",
      "ç¬¬4è½®:\n",
      "ç”¨æˆ·: æˆ‘å–œæ¬¢ä»€ä¹ˆä¹¦ï¼Ÿ\n",
      "AI: AI: ä½ å–œæ¬¢ã€Šä¸‰ä½“ã€‹å•Šï¼Œçœ‹æ¥ä½ å¯¹ç§‘å¹»å°è¯´éå¸¸æ„Ÿå…´è¶£ï¼ ğŸ˜Š  ä¹‹å‰æˆ‘ä»¬èŠè¿‡ä½ å–œæ¬¢é˜…è¯»ï¼Œè€Œä¸”ä½ ç‰¹åˆ«å–œæ¬¢ã€Šä¸‰ä½“ã€‹ã€‚  ä½ è§‰å¾—ã€Šä¸‰ä½“ã€‹çš„ä»€ä¹ˆå¸å¼•äº†ä½ å‘¢ï¼Ÿ è¿˜æ˜¯è¯´ï¼Œä½ è¿˜è®°å¾—æˆ‘ä»¬ä¹‹å‰èŠè¿‡ä½ å–œæ¬¢é˜…è¯»è¿™ä»¶äº‹å‘¢ï¼Ÿ ğŸ˜‰\n",
      "\n",
      "ä¼šè¯å†å²: 8 æ¡æ¶ˆæ¯\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ===================== æ–¹æ³•3: å¸¦æœ‰è®°å¿†çª—å£çš„LCELé“¾ =====================\n",
    "\n",
    "print(\"\\n\\n3. å¸¦æœ‰è®°å¿†çª—å£çš„LCELé“¾\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "class WindowMemoryChain:\n",
    "    \"\"\"å¸¦æœ‰çª—å£è®°å¿†çš„é“¾\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, window_size: int = 6):\n",
    "        self.llm = llm\n",
    "        self.window_size = window_size  # ä¿ç•™çš„æ¶ˆæ¯æ•°é‡\n",
    "        self.history: List[BaseMessage] = []\n",
    "        \n",
    "        # åˆ›å»ºæç¤ºæ¨¡æ¿\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ã€‚åŸºäºæœ€è¿‘çš„å¯¹è¯å†å²è¿›è¡Œå›å¤ã€‚\"),\n",
    "            MessagesPlaceholder(variable_name=\"recent_history\"),\n",
    "            (\"human\", \"{input}\")\n",
    "        ])\n",
    "        \n",
    "        # åˆ›å»ºå¤„ç†å‡½æ•°\n",
    "        def get_recent_history(inputs: dict) -> dict:\n",
    "            \"\"\"è·å–æœ€è¿‘çš„å†å²è®°å½•\"\"\"\n",
    "            recent = self.history[-self.window_size:] if len(self.history) > self.window_size else self.history\n",
    "            return {\n",
    "                \"recent_history\": recent,\n",
    "                \"input\": inputs[\"input\"]\n",
    "            }\n",
    "        \n",
    "        # åˆ›å»ºé“¾\n",
    "        self.chain = (\n",
    "            RunnableLambda(get_recent_history) |\n",
    "            self.prompt |\n",
    "            self.llm |\n",
    "            StrOutputParser()\n",
    "        )\n",
    "    \n",
    "    def invoke(self, user_input: str) -> str:\n",
    "        \"\"\"è°ƒç”¨é“¾\"\"\"\n",
    "        response = self.chain.invoke({\"input\": user_input})\n",
    "        \n",
    "        # æ›´æ–°å†å²\n",
    "        self.history.append(HumanMessage(content=user_input))\n",
    "        self.history.append(AIMessage(content=response))\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def get_window_info(self) -> dict:\n",
    "        \"\"\"è·å–çª—å£ä¿¡æ¯\"\"\"\n",
    "        return {\n",
    "            \"total_messages\": len(self.history),\n",
    "            \"window_size\": self.window_size,\n",
    "            \"messages_in_window\": min(len(self.history), self.window_size)\n",
    "        }\n",
    "\n",
    "def demo_window_memory():\n",
    "    \"\"\"æ¼”ç¤ºçª—å£è®°å¿†\"\"\"\n",
    "    print(\"\\nçª—å£è®°å¿†æ¼”ç¤º (çª—å£å¤§å°=4):\")\n",
    "    \n",
    "    chain = WindowMemoryChain(llm, window_size=4)\n",
    "    \n",
    "    conversations = [\n",
    "        \"æˆ‘å«å¼ ä¸‰ï¼Œæ˜¯åŒ»ç”Ÿ\",\n",
    "        \"æˆ‘åœ¨åŒ—äº¬å·¥ä½œ\",\n",
    "        \"æˆ‘å–œæ¬¢æ¸¸æ³³\",\n",
    "        \"æˆ‘æœ‰ä¸€åªçŒ«\",\n",
    "        \"æˆ‘çš„çŒ«å«å°ç™½\",\n",
    "        \"æˆ‘è¿˜å–œæ¬¢çœ‹ç”µå½±\",\n",
    "        \"ä½ è¿˜è®°å¾—æˆ‘çš„åå­—å—ï¼Ÿ\",  # å¯èƒ½å·²ç»è¶…å‡ºçª—å£\n",
    "        \"æˆ‘çš„å® ç‰©æ˜¯ä»€ä¹ˆï¼Ÿ\",      # åº”è¯¥è¿˜è®°å¾—\n",
    "    ]\n",
    "    \n",
    "    for i, user_input in enumerate(conversations, 1):\n",
    "        try:\n",
    "            response = chain.invoke(user_input)\n",
    "            print(f\"\\nç¬¬{i}è½®:\")\n",
    "            print(f\"ç”¨æˆ·: {user_input}\")\n",
    "            print(f\"AI: {response}\")\n",
    "            \n",
    "            # æ˜¾ç¤ºçª—å£ä¿¡æ¯\n",
    "            info = chain.get_window_info()\n",
    "            print(f\"çª—å£çŠ¶æ€: {info['messages_in_window']}/{info['window_size']} (æ€»è®¡: {info['total_messages']})\")\n",
    "        except Exception as e:\n",
    "            print(f\"é”™è¯¯: {e}\")\n",
    "\n",
    "demo_window_memory()\n"
   ],
   "id": "bd75e622ad02465b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ===================== æ–¹æ³•4: å¸¦æœ‰æ‘˜è¦åŠŸèƒ½çš„LCELé“¾ =====================\n",
    "\n",
    "print(\"\\n\\n4. å¸¦æœ‰æ‘˜è¦åŠŸèƒ½çš„LCELé“¾\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "class SummaryMemoryChain:\n",
    "    \"\"\"å¸¦æœ‰æ‘˜è¦åŠŸèƒ½çš„è®°å¿†é“¾\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, max_messages: int = 8):\n",
    "        self.llm = llm\n",
    "        self.max_messages = max_messages\n",
    "        self.history: List[BaseMessage] = []\n",
    "        self.summary: str = \"\"\n",
    "        \n",
    "        # ä¸»å¯¹è¯æç¤º\n",
    "        self.chat_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ã€‚\n",
    "\n",
    "{summary_context}\n",
    "\n",
    "è¯·åŸºäºå¯¹è¯æ‘˜è¦å’Œæœ€è¿‘çš„å¯¹è¯å†å²è¿›è¡Œå›å¤ã€‚\"\"\"),\n",
    "            MessagesPlaceholder(variable_name=\"recent_history\"),\n",
    "            (\"human\", \"{input}\")\n",
    "        ])\n",
    "        \n",
    "        # æ‘˜è¦æç¤º\n",
    "        self.summary_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"è¯·ç®€æ´åœ°æ€»ç»“ä»¥ä¸‹å¯¹è¯çš„è¦ç‚¹ï¼Œä¿ç•™é‡è¦ä¿¡æ¯ï¼š\"),\n",
    "            MessagesPlaceholder(variable_name=\"messages_to_summarize\"),\n",
    "            (\"human\", \"è¯·æä¾›å¯¹è¯æ‘˜è¦ï¼š\")\n",
    "        ])\n",
    "        \n",
    "        # åˆ›å»ºé“¾\n",
    "        self.chat_chain = self.chat_prompt | self.llm | StrOutputParser()\n",
    "        self.summary_chain = self.summary_prompt | self.llm | StrOutputParser()\n",
    "    \n",
    "    def _create_summary(self, messages: List[BaseMessage]) -> str:\n",
    "        \"\"\"åˆ›å»ºå¯¹è¯æ‘˜è¦\"\"\"\n",
    "        try:\n",
    "            return self.summary_chain.invoke({\n",
    "                \"messages_to_summarize\": messages\n",
    "            })\n",
    "        except:\n",
    "            return \"å¯¹è¯æ‘˜è¦ç”Ÿæˆå¤±è´¥\"\n",
    "    \n",
    "    def invoke(self, user_input: str) -> str:\n",
    "        \"\"\"è°ƒç”¨é“¾\"\"\"\n",
    "        # å¦‚æœå†å²å¤ªé•¿ï¼Œåˆ›å»ºæ‘˜è¦\n",
    "        if len(self.history) >= self.max_messages:\n",
    "            # æ‘˜è¦å‰é¢çš„æ¶ˆæ¯\n",
    "            messages_to_summarize = self.history[:-4]  # ä¿ç•™æœ€è¿‘2è½®å¯¹è¯\n",
    "            new_summary = self._create_summary(messages_to_summarize)\n",
    "            \n",
    "            # æ›´æ–°æ‘˜è¦å’Œå†å²\n",
    "            if self.summary:\n",
    "                self.summary = f\"{self.summary}\\n\\n{new_summary}\"\n",
    "            else:\n",
    "                self.summary = new_summary\n",
    "            \n",
    "            # åªä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯\n",
    "            self.history = self.history[-4:]\n",
    "        \n",
    "        # å‡†å¤‡ä¸Šä¸‹æ–‡\n",
    "        summary_context = f\"å¯¹è¯æ‘˜è¦: {self.summary}\" if self.summary else \"è¿™æ˜¯å¯¹è¯çš„å¼€å§‹ã€‚\"\n",
    "        \n",
    "        # è°ƒç”¨èŠå¤©é“¾\n",
    "        response = self.chat_chain.invoke({\n",
    "            \"summary_context\": summary_context,\n",
    "            \"recent_history\": self.history,\n",
    "            \"input\": user_input\n",
    "        })\n",
    "        \n",
    "        # æ›´æ–°å†å²\n",
    "        self.history.append(HumanMessage(content=user_input))\n",
    "        self.history.append(AIMessage(content=response))\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def get_memory_info(self) -> dict:\n",
    "        \"\"\"è·å–è®°å¿†ä¿¡æ¯\"\"\"\n",
    "        return {\n",
    "            \"recent_messages\": len(self.history),\n",
    "            \"has_summary\": bool(self.summary),\n",
    "            \"summary_length\": len(self.summary) if self.summary else 0\n",
    "        }\n",
    "\n",
    "def demo_summary_memory():\n",
    "    \"\"\"æ¼”ç¤ºæ‘˜è¦è®°å¿†\"\"\"\n",
    "    print(\"\\næ‘˜è¦è®°å¿†æ¼”ç¤º (æœ€å¤§æ¶ˆæ¯æ•°=6):\")\n",
    "    \n",
    "    chain = SummaryMemoryChain(llm, max_messages=6)\n",
    "    \n",
    "    conversations = [\n",
    "        \"æˆ‘å«ç‹äº”ï¼Œæ˜¯ä¸€åè€å¸ˆ\",\n",
    "        \"æˆ‘åœ¨ä¸Šæµ·çš„å°å­¦æ•™æ•°å­¦\",\n",
    "        \"æˆ‘æœ‰10å¹´çš„æ•™å­¦ç»éªŒ\",\n",
    "        \"æˆ‘å–œæ¬¢å’Œå­¦ç”Ÿäº’åŠ¨\",\n",
    "        \"æˆ‘çš„ç­çº§æœ‰30ä¸ªå­¦ç”Ÿ\",\n",
    "        \"æˆ‘æœ€è¿‘åœ¨å‡†å¤‡æœŸæœ«è€ƒè¯•\",\n",
    "        \"æˆ‘è¿˜è´Ÿè´£å­¦æ ¡çš„æ•°å­¦ç«èµ›\",\n",
    "        \"ä½ è¿˜è®°å¾—æˆ‘çš„èŒä¸šå—ï¼Ÿ\",\n",
    "        \"æˆ‘åœ¨å“ªé‡Œå·¥ä½œï¼Ÿ\",\n",
    "        \"æˆ‘æœ‰å¤šå°‘å¹´ç»éªŒï¼Ÿ\"\n",
    "    ]\n",
    "    \n",
    "    for i, user_input in enumerate(conversations, 1):\n",
    "        try:\n",
    "            response = chain.invoke(user_input)\n",
    "            print(f\"\\nç¬¬{i}è½®:\")\n",
    "            print(f\"ç”¨æˆ·: {user_input}\")\n",
    "            print(f\"AI: {response}\")\n",
    "            \n",
    "            # æ˜¾ç¤ºè®°å¿†ä¿¡æ¯\n",
    "            info = chain.get_memory_info()\n",
    "            print(f\"è®°å¿†çŠ¶æ€: {info['recent_messages']} æ¡æœ€è¿‘æ¶ˆæ¯, æ‘˜è¦: {'æœ‰' if info['has_summary'] else 'æ— '}\")\n",
    "            \n",
    "            if info['has_summary'] and i % 3 == 0:  # æ¯3è½®æ˜¾ç¤ºä¸€æ¬¡æ‘˜è¦\n",
    "                print(f\"å½“å‰æ‘˜è¦: {chain.summary[:100]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"é”™è¯¯: {e}\")\n",
    "\n",
    "demo_summary_memory()\n"
   ],
   "id": "88a67e0a03898d8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ===================== æ–¹æ³•5: å¤šä¼šè¯ç®¡ç†çš„LCELå®ç° =====================\n",
    "\n",
    "print(\"\\n\\n5. å¤šä¼šè¯ç®¡ç†çš„LCELå®ç°\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "class MultiSessionMemory:\n",
    "    \"\"\"å¤šä¼šè¯è®°å¿†ç®¡ç†\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.sessions: Dict[str, List[BaseMessage]] = {}\n",
    "        \n",
    "        # åˆ›å»ºæç¤ºæ¨¡æ¿\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ã€‚è¯·åŸºäºå¯¹è¯å†å²è¿›è¡Œä¸ªæ€§åŒ–å›å¤ã€‚\"),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            (\"human\", \"{input}\")\n",
    "        ])\n",
    "        \n",
    "        # åˆ›å»ºé“¾\n",
    "        self.chain = self.prompt | self.llm | StrOutputParser()\n",
    "    \n",
    "    def invoke(self, user_input: str, session_id: str) -> str:\n",
    "        \"\"\"ä¸ºç‰¹å®šä¼šè¯è°ƒç”¨é“¾\"\"\"\n",
    "        # è·å–æˆ–åˆ›å»ºä¼šè¯å†å²\n",
    "        if session_id not in self.sessions:\n",
    "            self.sessions[session_id] = []\n",
    "        \n",
    "        history = self.sessions[session_id]\n",
    "        \n",
    "        # è°ƒç”¨é“¾\n",
    "        response = self.chain.invoke({\n",
    "            \"history\": history,\n",
    "            \"input\": user_input\n",
    "        })\n",
    "        \n",
    "        # æ›´æ–°ä¼šè¯å†å²\n",
    "        history.append(HumanMessage(content=user_input))\n",
    "        history.append(AIMessage(content=response))\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def get_session_info(self) -> dict:\n",
    "        \"\"\"è·å–ä¼šè¯ä¿¡æ¯\"\"\"\n",
    "        return {\n",
    "            \"total_sessions\": len(self.sessions),\n",
    "            \"session_ids\": list(self.sessions.keys()),\n",
    "            \"messages_per_session\": {\n",
    "                sid: len(messages) for sid, messages in self.sessions.items()\n",
    "            }\n",
    "        }\n",
    "\n",
    "def demo_multi_session():\n",
    "    \"\"\"æ¼”ç¤ºå¤šä¼šè¯ç®¡ç†\"\"\"\n",
    "    print(\"\\nå¤šä¼šè¯ç®¡ç†æ¼”ç¤º:\")\n",
    "    \n",
    "    memory = MultiSessionMemory(llm)\n",
    "    \n",
    "    # ç”¨æˆ·Açš„å¯¹è¯\n",
    "    print(\"\\n=== ç”¨æˆ·Açš„ä¼šè¯ ===\")\n",
    "    response1 = memory.invoke(\"æˆ‘æ˜¯Aliceï¼Œæˆ‘å–œæ¬¢ç”»ç”»\", \"user_a\")\n",
    "    print(f\"Alice: æˆ‘æ˜¯Aliceï¼Œæˆ‘å–œæ¬¢ç”»ç”»\")\n",
    "    print(f\"AI: {response1}\")\n",
    "    \n",
    "    # ç”¨æˆ·Bçš„å¯¹è¯\n",
    "    print(\"\\n=== ç”¨æˆ·Bçš„ä¼šè¯ ===\")\n",
    "    response2 = memory.invoke(\"æˆ‘æ˜¯Bobï¼Œæˆ‘æ˜¯ç¨‹åºå‘˜\", \"user_b\")\n",
    "    print(f\"Bob: æˆ‘æ˜¯Bobï¼Œæˆ‘æ˜¯ç¨‹åºå‘˜\")\n",
    "    print(f\"AI: {response2}\")\n",
    "    \n",
    "    # ç»§ç»­ç”¨æˆ·Açš„å¯¹è¯\n",
    "    print(\"\\n=== ç»§ç»­Aliceçš„ä¼šè¯ ===\")\n",
    "    response3 = memory.invoke(\"ä½ è®°å¾—æˆ‘çš„çˆ±å¥½å—ï¼Ÿ\", \"user_a\")\n",
    "    print(f\"Alice: ä½ è®°å¾—æˆ‘çš„çˆ±å¥½å—ï¼Ÿ\")\n",
    "    print(f\"AI: {response3}\")\n",
    "    \n",
    "    # ç»§ç»­ç”¨æˆ·Bçš„å¯¹è¯\n",
    "    print(\"\\n=== ç»§ç»­Bobçš„ä¼šè¯ ===\")\n",
    "    response4 = memory.invoke(\"æˆ‘çš„èŒä¸šæ˜¯ä»€ä¹ˆï¼Ÿ\", \"user_b\")\n",
    "    print(f\"Bob: æˆ‘çš„èŒä¸šæ˜¯ä»€ä¹ˆï¼Ÿ\")\n",
    "    print(f\"AI: {response4}\")\n",
    "    \n",
    "    # æ˜¾ç¤ºä¼šè¯ä¿¡æ¯\n",
    "    info = memory.get_session_info()\n",
    "    print(f\"\\nä¼šè¯ç»Ÿè®¡: {info}\")\n",
    "\n",
    "demo_multi_session()\n"
   ],
   "id": "a2ace54b3ee32d97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ===================== æ€»ç»“å’Œå¯¹æ¯” =====================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"LCEL å¯¹è¯è®°å¿†å®ç°æ–¹æ³•æ€»ç»“\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary_text = \"\"\"\n",
    "LCEL å®ç°å¯¹è¯è®°å¿†çš„æ–¹æ³•:\n",
    "\n",
    "1. ğŸ”§ æ‰‹åŠ¨ç®¡ç†å†å²\n",
    "   ä¼˜ç‚¹: å®Œå…¨æ§åˆ¶ï¼Œç®€å•ç›´æ¥\n",
    "   ç¼ºç‚¹: éœ€è¦æ‰‹åŠ¨ç®¡ç†çŠ¶æ€\n",
    "   é€‚ç”¨: ç®€å•åº”ç”¨ï¼Œå­¦ä¹ ç›®çš„\n",
    "\n",
    "2. ğŸ“š RunnableWithMessageHistory\n",
    "   ä¼˜ç‚¹: å®˜æ–¹æ”¯æŒï¼ŒåŠŸèƒ½å®Œæ•´\n",
    "   ç¼ºç‚¹: APIç›¸å¯¹å¤æ‚\n",
    "   é€‚ç”¨: æ ‡å‡†èŠå¤©åº”ç”¨\n",
    "\n",
    "3. ğŸªŸ çª—å£è®°å¿†\n",
    "   ä¼˜ç‚¹: å†…å­˜ä½¿ç”¨å›ºå®š\n",
    "   ç¼ºç‚¹: ä¼šä¸¢å¤±æ—©æœŸä¿¡æ¯\n",
    "   é€‚ç”¨: é•¿å¯¹è¯ï¼Œå†…å­˜æœ‰é™\n",
    "\n",
    "4. ğŸ“ æ‘˜è¦è®°å¿†\n",
    "   ä¼˜ç‚¹: ä¿ç•™é‡è¦ä¿¡æ¯ï¼Œé€‚åˆé•¿å¯¹è¯\n",
    "   ç¼ºç‚¹: éœ€è¦é¢å¤–LLMè°ƒç”¨\n",
    "   é€‚ç”¨: è¶…é•¿å¯¹è¯ï¼Œæˆæœ¬æ•æ„Ÿ\n",
    "\n",
    "5. ğŸ‘¥ å¤šä¼šè¯ç®¡ç†\n",
    "   ä¼˜ç‚¹: æ”¯æŒå¤šç”¨æˆ·\n",
    "   ç¼ºç‚¹: éœ€è¦æ‰‹åŠ¨å®ç°ä¼šè¯éš”ç¦»\n",
    "   é€‚ç”¨: å¤šç”¨æˆ·åº”ç”¨\n",
    "\n",
    "é€‰æ‹©å»ºè®®:\n",
    "- ç®€å•èŠå¤©: æ–¹æ³•1æˆ–2\n",
    "- é•¿å¯¹è¯: æ–¹æ³•3æˆ–4\n",
    "- å¤šç”¨æˆ·: æ–¹æ³•5\n",
    "- å¤æ‚åº”ç”¨: è€ƒè™‘è¿ç§»åˆ°LangGraph\n",
    "\n",
    "æ³¨æ„: LangChain 0.3+ æ¨èä½¿ç”¨ LangGraph æ¥å¤„ç†å¤æ‚çš„è®°å¿†éœ€æ±‚ï¼\n",
    "\"\"\"\n",
    "\n",
    "print(summary_text)\n",
    "\n",
    "print(\"\\næ¼”ç¤ºå®Œæˆï¼\")\n",
    "print(\"è™½ç„¶LCELå¯ä»¥å®ç°è®°å¿†åŠŸèƒ½ï¼Œä½†å¯¹äºå¤æ‚åº”ç”¨å»ºè®®ä½¿ç”¨LangGraphã€‚\")\n"
   ],
   "id": "5718b4a843cc784e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ===================== äº¤äº’å¼æ¼”ç¤º =====================\n",
    "\n",
    "def interactive_lcel_demo():\n",
    "    \"\"\"äº¤äº’å¼LCELè®°å¿†æ¼”ç¤º\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"äº¤äº’å¼LCELè®°å¿†æ¼”ç¤º\")\n",
    "    print(\"é€‰æ‹©ä¸€ç§è®°å¿†å®ç°æ–¹å¼è¿›è¡Œæµ‹è¯•\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    print(\"\\nå¯ç”¨çš„è®°å¿†å®ç°:\")\n",
    "    print(\"1. æ‰‹åŠ¨ç®¡ç†å†å²\")\n",
    "    print(\"2. RunnableWithMessageHistory\")\n",
    "    print(\"3. çª—å£è®°å¿†\")\n",
    "    print(\"4. æ‘˜è¦è®°å¿†\")\n",
    "    print(\"5. å¤šä¼šè¯ç®¡ç†\")\n",
    "\n",
    "    choice = input(\"\\nè¯·é€‰æ‹©å®ç°æ–¹å¼ (1-5): \").strip()\n",
    "\n",
    "    if choice == \"1\":\n",
    "        chain = ManualMemoryChain(llm)\n",
    "        print(\"\\nä½¿ç”¨æ‰‹åŠ¨ç®¡ç†å†å²ï¼Œè¾“å…¥ 'quit' é€€å‡º\")\n",
    "\n",
    "        while True:\n",
    "            user_input = input(\"\\nä½ : \").strip()\n",
    "            if user_input.lower() == 'quit':\n",
    "                break\n",
    "            try:\n",
    "                response = chain.invoke(user_input)\n",
    "                print(f\"AI: {response}\")\n",
    "            except Exception as e:\n",
    "                print(f\"é”™è¯¯: {e}\")\n",
    "\n",
    "    elif choice == \"2\":\n",
    "        config = {\"configurable\": {\"session_id\": \"interactive\"}}\n",
    "        print(\"\\nä½¿ç”¨ RunnableWithMessageHistoryï¼Œè¾“å…¥ 'quit' é€€å‡º\")\n",
    "\n",
    "        while True:\n",
    "            user_input = input(\"\\nä½ : \").strip()\n",
    "            if user_input.lower() == 'quit':\n",
    "                break\n",
    "            try:\n",
    "                response = chain_with_history.invoke(\n",
    "                    {\"input\": user_input}, config=config\n",
    "                )\n",
    "                print(f\"AI: {response}\")\n",
    "            except Exception as e:\n",
    "                print(f\"é”™è¯¯: {e}\")\n",
    "\n",
    "    elif choice == \"3\":\n",
    "        chain = WindowMemoryChain(llm, window_size=6)\n",
    "        print(\"\\nä½¿ç”¨çª—å£è®°å¿† (çª—å£å¤§å°=6)ï¼Œè¾“å…¥ 'quit' é€€å‡º\")\n",
    "\n",
    "        while True:\n",
    "            user_input = input(\"\\nä½ : \").strip()\n",
    "            if user_input.lower() == 'quit':\n",
    "                break\n",
    "            try:\n",
    "                response = chain.invoke(user_input)\n",
    "                print(f\"AI: {response}\")\n",
    "                info = chain.get_window_info()\n",
    "                print(f\"[çª—å£: {info['messages_in_window']}/{info['window_size']}]\")\n",
    "            except Exception as e:\n",
    "                print(f\"é”™è¯¯: {e}\")\n",
    "\n",
    "    elif choice == \"4\":\n",
    "        chain = SummaryMemoryChain(llm, max_messages=8)\n",
    "        print(\"\\nä½¿ç”¨æ‘˜è¦è®°å¿† (æœ€å¤§æ¶ˆæ¯=8)ï¼Œè¾“å…¥ 'quit' é€€å‡º\")\n",
    "\n",
    "        while True:\n",
    "            user_input = input(\"\\nä½ : \").strip()\n",
    "            if user_input.lower() == 'quit':\n",
    "                break\n",
    "            try:\n",
    "                response = chain.invoke(user_input)\n",
    "                print(f\"AI: {response}\")\n",
    "                info = chain.get_memory_info()\n",
    "                print(f\"[æ¶ˆæ¯: {info['recent_messages']}, æ‘˜è¦: {'æœ‰' if info['has_summary'] else 'æ— '}]\")\n",
    "            except Exception as e:\n",
    "                print(f\"é”™è¯¯: {e}\")\n",
    "\n",
    "    elif choice == \"5\":\n",
    "        memory = MultiSessionMemory(llm)\n",
    "        session_id = input(\"è¯·è¾“å…¥ä¼šè¯ID: \").strip() or \"default\"\n",
    "        print(f\"\\nä½¿ç”¨å¤šä¼šè¯ç®¡ç† (ä¼šè¯: {session_id})ï¼Œè¾“å…¥ 'quit' é€€å‡ºï¼Œè¾“å…¥ 'switch' åˆ‡æ¢ä¼šè¯\")\n",
    "\n",
    "        while True:\n",
    "            user_input = input(\"\\nä½ : \").strip()\n",
    "            if user_input.lower() == 'quit':\n",
    "                break\n",
    "            elif user_input.lower() == 'switch':\n",
    "                session_id = input(\"è¯·è¾“å…¥æ–°çš„ä¼šè¯ID: \").strip() or \"default\"\n",
    "                print(f\"åˆ‡æ¢åˆ°ä¼šè¯: {session_id}\")\n",
    "                continue\n",
    "            try:\n",
    "                response = memory.invoke(user_input, session_id)\n",
    "                print(f\"AI: {response}\")\n",
    "                info = memory.get_session_info()\n",
    "                print(f\"[ä¼šè¯: {session_id}, æ€»ä¼šè¯æ•°: {info['total_sessions']}]\")\n",
    "            except Exception as e:\n",
    "                print(f\"é”™è¯¯: {e}\")\n",
    "    else:\n",
    "        print(\"æ— æ•ˆé€‰æ‹©\")\n",
    "\n",
    "# å¦‚æœç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶ï¼Œå¯åŠ¨äº¤äº’å¼æ¼”ç¤º\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # æµ‹è¯•è¿æ¥\n",
    "        test_response = llm.invoke(\"Hello\")\n",
    "        print(\"âœ“ Ollama è¿æ¥æˆåŠŸ\")\n",
    "        interactive_lcel_demo()\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Ollama è¿æ¥å¤±è´¥: {e}\")\n",
    "        print(\"è¯·ç¡®ä¿ Ollama æ­£åœ¨è¿è¡Œ: ollama serve\")\n",
    "        print(\"å¹¶å®‰è£…æ¨¡å‹: ollama pull gemma:3b\")\n"
   ],
   "id": "40306f2d82d186f7"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
